{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Corpus Linguistics\n",
    "\n",
    "This week, we take text corpora that we have developed, spidered, scraped, and encoded, and we find and count words and simple phrases and properties of those counts (e.g., word frequency distributions). Initially, we model how to search corpora for keywords or phrases. Next, we examine the distributions of terms and phrases across a corpus, and the corelation between different words and phrase counts. In order to do this effectively, we coarsely disambiguate words based of part-of-speech (POS) tagging, and normalize them through stemming and lemmatization. Next we distinguish *important* words and phrase within the corpus, and image them with Wordls! Then we calculate word frequenceis, conditional frequences (the frequency of word *shock* conditional on the presence of word *awe*), and statistically significant collocations of lengths 2 through $n$. Finally, we calculate and visualize Differences (Divergences and Distances) between the word frequency distributions from two corpora. \n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "import lucem_illud #pip install git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import nltk #the Natural Language Toolkit\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud #Makes word clouds\n",
    "import numpy as np #For divergences/distances\n",
    "import scipy #For divergences/distances\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "import sklearn.manifold #For a manifold plot\n",
    "from nltk.corpus import stopwords #For stopwords\n",
    "import json #For API responses\n",
    "import urllib.parse #For joining urls\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving exemplary corpora\n",
    "\n",
    "To get started we will need some examples. Let's start by downloading one of the corpuses from `nltk`, a Natural Language Toolkit developed by computational linguists at the University of Pennsylvania. Let's take a look at how that works.\n",
    "\n",
    "First we can get a list of works available from the Gutenburg corpus, with the [corpus module](http://www.nltk.org/api/nltk.corpus.html). To do this we will need to tell nltk where the data are, and download them if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You should have them already downloaded in the data directory\n",
    "try:\n",
    "    #adding path for both local and server, only one of these will actually be used at a time\n",
    "    nltk.data.path.append('/project2/macs60000/shared_data/nltk')\n",
    "    nltk.data.path.append('../data')\n",
    "    #Check that everything is in place\n",
    "    nltk.corpus.gutenberg.fileids()\n",
    "except LookupError:\n",
    "    print(\"You have to download all the documents\")\n",
    "    print(\"Downloading to ../data this should only take a couple minutes\")\n",
    "    nltk.download('all', download_dir = '../data')\n",
    "    nltk.data.path.append('../data')\n",
    "#If you haven't downloaded the copura then you will have to run `nltk.download()`\n",
    "print(nltk.corpus.gutenberg.fileids())\n",
    "print(len(nltk.corpus.gutenberg.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the individual works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')[2000:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the listed works have been nicely marked up and classified for us so we can do much better than just looking at raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'))\n",
    "print(nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words\n",
    "\n",
    "If we want to do some analysis we can start by simply counting the number of times each word occurs within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordCounter(wordLst):\n",
    "    wordCounts = {}\n",
    "    for word in wordLst:\n",
    "        #We usually need to normalize the case\n",
    "        wLower = word.lower()\n",
    "        if wLower in wordCounts:\n",
    "            wordCounts[wLower] += 1\n",
    "        else:\n",
    "            wordCounts[wLower] = 1\n",
    "    #convert to DataFrame\n",
    "    countsForFrame = {'word' : [], 'count' : []}\n",
    "    for w, c in wordCounts.items():\n",
    "        countsForFrame['word'].append(w)\n",
    "        countsForFrame['count'].append(c)\n",
    "    return pandas.DataFrame(countsForFrame)\n",
    "\n",
    "countedWords = wordCounter(nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'))\n",
    "countedWords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `wordCounter()` is not a very complicated function. That is because the hard parts have already been done by `nltk`. If we were using unprocessed text we would have to tokenize and determine what to do with the non-word characters.\n",
    "\n",
    "nltk also offers a built-in way for getting a frequency distribution from a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [word.lower() for word in nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')]\n",
    "freq = nltk.FreqDist(words)\n",
    "print (freq['macbeth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets sort and plot our counts to investigate the shape of our word frequency distribution.\n",
    "\n",
    "First we need to sort the words by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Doing this in place (changing the original DataFrame) as we don't need the unsorted DataFrame\n",
    "countedWords.sort_values('count', ascending=False, inplace=True)\n",
    "countedWords[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation and very common words (articles 'a' and 'the'; prepositions 'of' and 'to') make up the most common values, but this isn't very interesting and can actually get in the way of our analysis. We may remove these 'function words' by removing according to a stopword list, setting some frequency threshold, or using a weighting scheme (like tf.idf) to decrease their influence (all modeled later in this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating a figure and axis lets us do things like change the scaling or add a title\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(range(len(countedWords)), countedWords['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that likelihood of a word occurring is inversely proportional to its rank. This effect is called [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law), and suggests that the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. Zipf's law is most easily observed by plotting the data on a log-log graph, with the axes being log (rank order) and log (frequency) resulting in a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(range(len(countedWords)), countedWords['count'])\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The same relationship occurs in many other rankings, such as population ranks of cities, corporation sizes, income rankings, etc.) \n",
    "\n",
    "The distribution was imagined by Zipf to be driven by a principle of 'least effort' where speakers did not work any harder than necessary to communicate a given idea, but the basis for this relationship is still not well understood and conforms at least as well to a process of [preferential attachment](https://en.wikipedia.org/wiki/Preferential_attachment) whereby people disproportionately attend to popular words.\n",
    "\n",
    "There are many other properties of words we can examine. First lets look surrounding words with the concordance. To do this we need to load the text into a `ConcordanceIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macbethIndex = nltk.text.ConcordanceIndex(nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can retrieve all the words that cooccur with a word in a given word window. Let's first look at `'macbeth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macbethIndex.print_concordance('macbeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird?! `'macbeth'` doesn't occur anywhere in the the text. What happened?\n",
    "\n",
    "`ConcordanceIndex` is case sensitive, lets try looking for `'Macbeth'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macbethIndex.print_concordance('Macbeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. What about something a lot less frequent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(countedWords[countedWords['word'] == 'donalbaine'])\n",
    "macbethIndex.print_concordance('Donalbaine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and working with raw text\n",
    "\n",
    "First, we will retrieve a collection of press releases from a *GitHub API* that archived them, based on a number of analyses by Justin Grimmer, a political scientist whose work we will read next week, and who will be joining the University of Chicago next July!\n",
    "\n",
    "GitHub API requests are made to `'https://api.github.com/'` and responses are in JSON, similar to Tumblr's API.\n",
    "\n",
    "We will get the information on [github.com/lintool/GrimmerSenatePressReleases](https://github.com/lintool/GrimmerSenatePressReleases) as it contains a nice set documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com/repos/lintool/GrimmerSenatePressReleases')\n",
    "senateReleasesData = json.loads(r.text)\n",
    "print(senateReleasesData.keys())\n",
    "print(senateReleasesData['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are interested in here is the `'contents_url'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(senateReleasesData['contents_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to get any or all of the files from the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse')\n",
    "whitehouseLinks = json.loads(r.text)\n",
    "whitehouseLinks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of information about Whitehouse press releases. Let's look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(whitehouseLinks[0]['download_url'])\n",
    "whitehouseRelease = r.text\n",
    "print(whitehouseRelease[:1000])\n",
    "len(whitehouseRelease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a string of correctly encoded text, similar to those we constructed from last week's assignment. For analysis of its words, next we need to tokenize it, or to split it into a sequence of tokens or word instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whTokens = nltk.word_tokenize(whitehouseRelease)\n",
    "whTokens[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`whTokens` is a list of 'words' constructed from nltk's `word_tokenize` method built on the [Penn Treebank tokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank). While it's better than `.split(' ')`, a general python string method that splits on whitespace, it is not perfect. There are many different ways to tokenize a string. `word_tokenize` is unfortunately unaware of sentences and is essentially a complicated regular expression (regex) run across the string.\n",
    "\n",
    "If we want to find sentences we can use something like `nltk.sent_tokenize()`, which implements the [Punkt Sentence tokenizer](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer), a machine learning based algorithm that works well for many European languages.\n",
    "\n",
    "We could also use the [Stanford tokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.stanford) or construct our own regex with [`RegexpTokenizer()`](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.regexp). Picking the correct tokenizer is important as the tokens form the base of our analysis.\n",
    "\n",
    "For our purposes in this example, the Penn Treebank tokenizer is fine.\n",
    "\n",
    "To use the list of tokens in `nltk`, and take advantage of functions like `concordance`, shown above, we can convert it into a `Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whText = nltk.Text(whTokens)\n",
    "\n",
    "whitehouseIndex = nltk.text.ConcordanceIndex(whText) \n",
    "whitehouseIndex.print_concordance('Whitehouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* that the `Text` class is for doing rapid, exploratory analysis. It provides an easy interface to many of the operations we want to do, but it does not allow us much control over the particular operations it glosses. When you are doing a more complete analysis, you should be using the module specifically designed for that task instead of the shortcut method `Text` provides, e.g. use  [`collocations` Module](http://www.nltk.org/api/nltk.html#module-nltk.collocations) instead of `.collocations()`.\n",
    "\n",
    "Now that we have gotten this loaded, let's glance at few features we will delve into more deeply later.\n",
    "\n",
    "For example, we can find words that statistically tend to occur together and typically have a composite, idiomatic meaning irreducible to the semantics of its component words. We will do this later with more control over exactly how these are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whText.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can pick a word (or words) and find what words tend to occur around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whText.common_contexts(['stem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just count the number of times the word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whText.count('cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also plot each time a set of words occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.reset_orig() #Seaborn messes with this plot, disabling it\n",
    "whText.dispersion_plot(['Sen.','stem', 'cell', 'federal' ,'Lila', 'Barber', 'Whitehouse'])\n",
    "sns.set() #Re-enabling seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do an analysis of all the Whitehouse press releases we will first need to obtain them. By looking at the API we can see the the URL we want is [https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse](https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse), so we can create a function to scrape the individual files.\n",
    "\n",
    "If you want to know more about downloading from APIs, refer back to the 1st notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getGithubFiles(target, maxFiles = 100):\n",
    "    #We are setting a max so our examples don't take too long to run\n",
    "    #For converting to a DataFrame\n",
    "    releasesDict = {\n",
    "        'name' : [], #The name of the file\n",
    "        'text' : [], #The text of the file, watch out for binary files\n",
    "        'path' : [], #The path in the git repo to the file\n",
    "        'html_url' : [], #The url to see the file on Github\n",
    "        'download_url' : [], #The url to download the file\n",
    "    }\n",
    "\n",
    "    #Get the directory information from Github\n",
    "    r = requests.get(target)\n",
    "    filesLst = json.loads(r.text)\n",
    "\n",
    "    for fileDict in filesLst[:maxFiles]:\n",
    "        #These are provided by the directory\n",
    "        releasesDict['name'].append(fileDict['name'])\n",
    "        releasesDict['path'].append(fileDict['path'])\n",
    "        releasesDict['html_url'].append(fileDict['html_url'])\n",
    "        releasesDict['download_url'].append(fileDict['download_url'])\n",
    "\n",
    "        #We need to download the text though\n",
    "        text = requests.get(fileDict['download_url']).text\n",
    "        releasesDict['text'].append(text)\n",
    "\n",
    "    return pandas.DataFrame(releasesDict)\n",
    "\n",
    "whReleases = getGithubFiles('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Whitehouse', maxFiles = 10)\n",
    "whReleases[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having trouble downloading the data uncomment this next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whReleases = pandas.read_csv('../data/whReleases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the texts in a DataFrame we can look at a few things.\n",
    "\n",
    "First let's tokenize the texts with the same tokenizer as we used before. We will just save the tokens as a list for now; no need to convert to `Text`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whReleases['tokenized_text'] = whReleases['text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how long each of the press releases is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whReleases['word_counts'] = whReleases['tokenized_text'].apply(lambda x: len(x))\n",
    "whReleases['word_counts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that input and interrogate a corpus relating to your anticipated final project. Turn your text into an nltk `Text` object, and explore all of the features examined above, and others that relate to better understanding your corpus in relation to your research question. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and normalizing text\n",
    "\n",
    "As we want to start comparing the different releases we may choose to do a bit of filtering and normalizing that will allow us to focus on what we most care about. We can first make all of the words lower case, then drop the non-word tokens. Next, we can remove some 'stop words', stem the remaining words to remove suffixes, prefixes and (in some languages) infixes, or lemmatize tokens by intelligently grouping inflected or variant forms of the same word (e.g., with a stemmer and a dictionary). \n",
    "\n",
    "To begin this process, we will first define a function to work over the tokenized lists, then another to add normalized tokens to a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk has a built-in list of stopwords. They are already imported in the import section. Let's first take a look at what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(', '.join(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove these stopwords from the analysis by fiat, but we could also take a more principled approach by looking at the frequency distribution of words and selecting a specific cut-off associated with the preservation of 'meaningful words' identified upon inspection. Alternatively, we could automatically set a cut-off by rule, such as removal of all words more frequent then the most frequent verb, or the most frequent noun (not pronoun), or some term of central interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate our own stop list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countsDict = {}\n",
    "for word in whReleases['tokenized_text'].sum():\n",
    "    if word in countsDict:\n",
    "        countsDict[word] += 1\n",
    "    else:\n",
    "        countsDict[word] = 1\n",
    "word_counts = sorted(countsDict.items(), key = lambda x : x[1], reverse = True)\n",
    "word_counts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at this and pick the cutoff, usually it is at the first noun. So we will cut all words before `'Whitehouse'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The stop list is then all words that occur before the first noun\n",
    "stop_words_freq = []\n",
    "for word, count in word_counts:\n",
    "    if word == 'Whitehouse':\n",
    "        break\n",
    "    else:\n",
    "        stop_words_freq.append(word)\n",
    "stop_words_freq\n",
    "wordnet = nltk.stem.WordNetLemmatizer()\n",
    "wordnet.lemmatize('are')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our function to normalize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words_nltk = stopwords.words('english')\n",
    "#stop_words = [\"the\",\"it\",\"she\",\"he\", \"a\"] #Uncomment this line if you want to use your own list of stopwords.\n",
    "\n",
    "#The stemmers and lemmers need to be initialized before bing run\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "wordnet = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "        \n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "    \n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "whReleases['normalized_tokens'] = whReleases['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = porter))\n",
    "\n",
    "whReleases['normalized_tokens_count'] = whReleases['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "whReleases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stemmer we use here is called the [Porter Stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter), there are many others, including another good one by the same person (Martin Porter) called the [Snowball Stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball) or 'another stemmer' called the [Lancaster Stemmer](http://www.nltk.org/_modules/nltk/stem/lancaster.html). We can also normalize our words by lemmatizing them, or matching word variants or inflected forms to a common morpheme in a dictionary like [WordNet](https://wordnet.princeton.edu) through intelligent stemming rules. (The WordNet function `morphy` returns the given word if it cannot be matched to a word in the dictionary).\n",
    "\n",
    "Now that it is cleaned we start analyzing the dataset. We can start by finding frequency distributions for the dataset. Lets start looking at all the press releases together. The [`ConditionalFreqDist`](http://www.nltk.org/api/nltk.html#nltk.probability.ConditionalProbDist) class reads in an iterable of tuples, the first element is the condition and the second the focal word. For starters, we will use word lengths as the conditions, but tags or clusters will provide more useful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#.sum() adds together the lists from each row into a single list\n",
    "whcfdist = nltk.ConditionalFreqDist(((len(w), w) for w in whReleases['normalized_tokens'].sum()))\n",
    "\n",
    "#print the number of words\n",
    "print(whcfdist.N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can lookup the distributions of different word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcfdist[3].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the most frequent 3-character word is \"thi\". But what is \"thi\"? It is actually \"this\" stemmed by the Porter Stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "print (porter.stem('this'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with the Snowball Stemer. See that \"this\" is corretly stemmed as a 4-character word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (snowball.stem('this'))\n",
    "\n",
    "whReleases['normalized_tokens'] = whReleases['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))\n",
    "whReleases['normalized_tokens_count'] = whReleases['normalized_tokens'].apply(lambda x: len(x))\n",
    "whcfdist = nltk.ConditionalFreqDist(((len(w), w) for w in whReleases['normalized_tokens'].sum()))\n",
    "whcfdist[3].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a Conditional Probability Distribution or [`ConditionalProbDist`](http://www.nltk.org/api/nltk.html#nltk.probability.ConditionalProbDist) from the `ConditionalFreqDist`. To do this, however, we need a model for the probability distribution. A simple model is [`ELEProbDist`](http://www.nltk.org/api/nltk.html#nltk.probability.ELEProbDist) which gives the expected likelihood estimate for the probability distribution of the experiment used to generate the observed frequency distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcpdist = nltk.ConditionalProbDist(whcfdist, nltk.ELEProbDist)\n",
    "\n",
    "#print the most common 2 letter word\n",
    "print(whcpdist[2].max())\n",
    "\n",
    "#And its probability\n",
    "print(whcpdist[2].prob(whcpdist[2].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length is one feature of a word, but there are many more important features we care about. Another critical feature is a word's role in the sentence, or its part of speech (POS). Here, we will be classifying words according to their part of speech (POS), using the [`nltk.pos_tag()`](http://www.nltk.org/api/nltk.tag.html#nltk.tag.pos_tag). The tags used here are those from the [Brown Corpus tagset](http://www.scs.leeds.ac.uk/amalgam/tagsets/brown.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whReleases['normalized_tokens_POS'] = [nltk.pos_tag(t) for t in whReleases['normalized_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a new column with the part of speech as a short initialism and the word in a tuple, exactly how the `nltk.ConditionalFreqDist()` function wants them. We can now construct another conditional frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcfdist_WordtoPOS = nltk.ConditionalFreqDist(whReleases['normalized_tokens_POS'].sum())\n",
    "list(whcfdist_WordtoPOS.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the frequency of each word as each part of speech...which can be uninformative and boring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcfdist_WordtoPOS['administr'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is the converse; the frequency of each part of speech for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whcfdist_POStoWord = nltk.ConditionalFreqDist((p, w) for w, p in whReleases['normalized_tokens_POS'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now identify and collect all of the superlative adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcfdist_POStoWord['JJS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or look at the most common nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcfdist_POStoWord['NN'].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or plot the base form verbs against their number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcfdist_POStoWord['VB'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then do a similar analysis of the word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whcpdist_POStoWord = nltk.ConditionalProbDist(whcfdist_POStoWord, nltk.ELEProbDist)\n",
    "\n",
    "#print the most common nouns\n",
    "print(whcpdist_POStoWord['NN'].max())\n",
    "\n",
    "#And its probability\n",
    "print(whcpdist_POStoWord['NN'].prob(whcpdist_POStoWord['NN'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a...wait for it...*WORD CLOUD* or Wordl to gaze at and draw mystical, approximate inferences about important nouns and verbs in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud(background_color=\"white\", max_words=500, width= 1000, height = 1000, mode ='RGBA', scale=.5).generate(' '.join(whReleases['normalized_tokens'].sum()))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"whitehouse_word_cloud.pdf\", format = 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that filter, stem and lemmatize the tokens in your corpus, and then creates plots (with titles and labels) that map the word frequency distribution, word probability distribution, and at least two conditional probability distributions that help us better understand the social and cultural game underlying the production of your corpus. Create a wordl of words (or normalized words) and add a few vague comments about what mysteries are revealed through it.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, we might want to find significant bigrams (two-word phrases), trigrams (three-word phrases), n-grams (*n*-word phrases) or skip-grams (noncontinguous 'phrases' with skip-length *n*). We will begin with the [`nltk.collocations.BigramCollocationFinder`](http://www.nltk.org/api/nltk.html?highlight=bigramcollocationfinder#nltk.collocations.BigramCollocationFinder) class, which can be given raw lists of strings with the `from_words()` method. By default it only looks at continuous bigrams but there is an option (`window_size`) to allow skip-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whBigrams = nltk.collocations.BigramCollocationFinder.from_words(whReleases['normalized_tokens'].sum())\n",
    "print(\"There are {} bigrams in the finder\".format(whBigrams.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the bigrams we need to tell nltk what our score function is. Initially, we will look at the raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigramScoring(count, wordsTuple, total):\n",
    "    return count\n",
    "\n",
    "print(whBigrams.nbest(bigramScoring, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note about how `BigramCollocationFinder` works. It doesn't use the strings internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "birgramScores = []\n",
    "\n",
    "def bigramPrinting(count, wordsTuple, total):\n",
    "    global birgramScores\n",
    "    birgramScores.append(\"The first word is:  {}, The second word is: {}\".format(*wordsTuple))\n",
    "    #Returns None so all the tuples are considered to have the same rank\n",
    "\n",
    "whBigrams.nbest(bigramPrinting, 10)\n",
    "print('\\n'.join(birgramScores[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are each given numeric IDs and there is a dictionary that maps the IDs to the words they represent. This is a common performance optimization.\n",
    "\n",
    "Two words can appear together by chance. Recall from  Manning and Schütze's textbook that a $t-value can be computed for each bigram to see how significant the association is. You may also want to try computing the $\\chi^2$, likelihood ratio, and pointwise mutual information statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "whBigrams.score_ngrams(bigram_measures.likelihood_ratio)[:40]\n",
    "# other options include student_t, chi_sq, likelihood_ratio, pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other available measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[s for s in dir(bigram_measures) if s[0] != '_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(whReleases['normalized_tokens'].sum())\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or n-grams (for any number n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ngrams = nltk.ngrams(whReleases['normalized_tokens'].sum(), 4)\n",
    "Counts = {}\n",
    "for ngram in list(Ngrams):\n",
    "    if ngram in Counts.keys():\n",
    "        Counts[ngram] += 1\n",
    "    else:\n",
    "        Counts[ngram] = 1\n",
    "Filtered = {}\n",
    "for key in Counts.keys():\n",
    "    if Counts[key] < 2:\n",
    "        pass\n",
    "    else:\n",
    "        Filtered[key] = Counts[key]\n",
    "print(Filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that identify statistically significant bigrams, trigrams, quadgrams, higher-order *n*grams and skipgrams. Explore whether these collocations are idiomatic and so irreducible to the semantic sum of their component words. You can do this by examination of conditional frequencies (e.g., what else is 'united' besides the 'United States'). If these phrases are idiomatic, what do they suggest about the culture of the world producing them?\n",
    "\n",
    "<span style=\"color:red\">**Stretch**: In Manning and Schütze's textbook, there Section 5.3.2 explores how to use the *t*-test to find words whose co-occurance patterns best distinguish two words. Implement that and use it to explore phrases in your corpus. For instance, you could tell what words come after \"America\" much more often than after \"Iraq\"?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional distances\n",
    "\n",
    "If we want to compare different corpora, we need a distance or divergence that compares the two distributions.\n",
    "\n",
    "We will use the: \n",
    "\n",
    "+ Kullback-Leibler (KL) divergence\n",
    "+ $\\chi^2$ divergence\n",
    "+ Kolmogorov-Smirnov (KS) distance\n",
    "+ Wasserstein distance\n",
    "\n",
    "### Kullback-Leibler and $x^2$ divergences ###\n",
    "\n",
    "KL and $\\chi^2$ divergences are members of the broader <a \"href=https://en.wikipedia.org/wiki/F-divergence\" target=\"_blank\">$f$-divergence</a> family, a function of $D_f (P || Q)$ that calculates the difference between two probability distributions P and Q. The KL $f(t)$ is $ t \\text{ log } t $, while the $\\chi^2$ is $t^2-1$. KL comes from information and $\\chi^2$ from measure theory. As such, the KL divergence computes the relative entropy between two distributions--how they differ in bits, while the $\\chi^2$ whether the same statistical inferences can be drawn from them both.  \n",
    "\n",
    "Specifically, given two discrete probability distributions $P$ and $Q$, the Kullback-Leibler divergence from $Q$ to $P$ is defined as:\n",
    "\n",
    "$D_{\\mathrm{KL}}(P\\|Q) = \\sum_i P(i) \\, \\log\\frac{P(i)}{Q(i)}$.\n",
    "\n",
    "The [scipy.stats.entropy()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html) function does the calculation for you, which takes in two arrays of probabilities and computes the KL divergence. Note that the KL divergence is in general not commutative, i.e. $D_{\\mathrm{KL}}(P\\|Q) \\neq D_{\\mathrm{KL}}(Q\\|P)$ .\n",
    "\n",
    "Also note that the KL divernce is the sum of elementwise divergences. Scipy provides [scipy.special.kl_div()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.kl_div.html#scipy-special-kl-div) which calculates elementwise divergences for you.\n",
    "\n",
    "The $\\chi^2$ Divergence is defined as:\n",
    "\n",
    "$D_{\\mathrm{\\chi^2}}(P\\|Q) = \\sum_i \\left(\\frac{P(i)}{Q(i)}-1\\right)^2$. \n",
    "\n",
    "This is also noncommutative, and the code can be drawn directly from scipy.\n",
    "\n",
    "### Kolmogorov-Smirnov ###\n",
    "\n",
    "The two-sample Kolmogovorov-Smirnov test statistic calculates the distance between the cumulative distribution function of the two distributions to be compared, and, along with the $x^2$ divergence, is among the most common approaches two calculating a distance in statistics. It can be interpreted as a test of whether two distributions are drawn from the same underlying distribution. As with the others, the code is readily available in scipy.\n",
    "\n",
    "### Wasserstein Distance ###\n",
    "\n",
    "When this is computed on a Euclidian metric structure (e.g., numbers of words), this is also known as the earth mover’s distance, because it can be seen as the minimum amount of \"work\" required to transform $P$ into $Q$, where \"work\" is measured as the amount of distribution weight that must be moved, multiplied by the distance it has to be moved.\n",
    "\n",
    "### Computing ###\n",
    "\n",
    "To do this we will need to create the arrays, lets compare the Whitehouse releases with the Kennedy releases. First we have to download them and load them into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kenReleases = getGithubFiles('https://api.github.com/repos/lintool/GrimmerSenatePressReleases/contents/raw/Kennedy', maxFiles = 10)\n",
    "kenReleases[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can tokenize, stem and remove stop words, like we did for the Whitehouse press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kenReleases['tokenized_text'] = kenReleases['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "kenReleases['normalized_tokens'] = kenReleases['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compare the two collection of words, remove those not found in both, and assign the remaining ones indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whWords = set(whReleases['normalized_tokens'].sum())\n",
    "kenWords = set(kenReleases['normalized_tokens'].sum())\n",
    "\n",
    "#Change & to | if you want to keep all words\n",
    "overlapWords = whWords & kenWords\n",
    "\n",
    "overlapWordsDict = {word: index for index, word in enumerate(overlapWords)}\n",
    "overlapWordsDict['student']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the occurrences of each word in the corpora and create our arrays. Note that we don't have to use numpy arrays as we do here. We could just use a list, but the arrays are faster in numpy so we encourage you to get in the habit of using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeProbsArray(dfColumn, overlapDict):\n",
    "    words = dfColumn.sum()\n",
    "    countList = [0] * len(overlapDict)\n",
    "    for word in words:\n",
    "        try:\n",
    "            countList[overlapDict[word]] += 1\n",
    "        except KeyError:\n",
    "            #The word is not common so we skip it\n",
    "            pass\n",
    "    countArray = np.array(countList)\n",
    "    return countArray / countArray.sum()\n",
    "\n",
    "whProbArray = makeProbsArray(whReleases['normalized_tokens'], overlapWordsDict)\n",
    "kenProbArray = makeProbsArray(kenReleases['normalized_tokens'], overlapWordsDict)\n",
    "kenProbArray.sum()\n",
    "#There is a little bit of a floating point math error\n",
    "#but it's too small to see with print and too small matter here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the KL divergence. Pay attention to the asymmetry. Use [the Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence), which is the average KL divergence between each distribution and the average of both distributions (i.e., the midpoint), if you want symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wh_kenDivergence = scipy.stats.entropy(whProbArray, kenProbArray)\n",
    "print (wh_kenDivergence)\n",
    "ken_whDivergence = scipy.stats.entropy(kenProbArray, whProbArray)\n",
    "print (ken_whDivergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can do the elementwise calculation and see which words best distinguish the two corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wh_kenDivergence_ew = scipy.special.kl_div(whProbArray, kenProbArray)\n",
    "kl_df = pandas.DataFrame(list(overlapWordsDict.keys()), columns = ['word'], index = list(overlapWordsDict.values()))\n",
    "kl_df = kl_df.sort_index()\n",
    "kl_df['elementwise divergence'] = wh_kenDivergence_ew\n",
    "kl_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kl_df.sort_values(by='elementwise divergence', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply what we learned today to the Guternberg texts in nltk and see if we can detect patterns between them. \n",
    "\n",
    "First, let's transform every text into normalized tokens. Note that in this first step, no stopword is removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileids = nltk.corpus.gutenberg.fileids()\n",
    "corpora = []\n",
    "for fileid in fileids:\n",
    "    words = nltk.corpus.gutenberg.words(fileid)\n",
    "    normalized_tokens = normlizeTokens(words, stopwordLst = [], stemmer = snowball)\n",
    "    corpora.append(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's separate the normalized tokens into stopwords and non-stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora_s = []\n",
    "corpora_nons = []\n",
    "for corpus in corpora:\n",
    "    s = []\n",
    "    nons = []\n",
    "    for word in corpus:\n",
    "        if word in stop_words_nltk:\n",
    "            s.append(word)\n",
    "        else:\n",
    "            nons.append(word)\n",
    "    corpora_s.append(s)\n",
    "    corpora_nons.append(nons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some covenient funtions for calculating divergence and distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pandas.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pandas.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the KL divergence for each pair of corpora, turn the results into a matrix, and visualize the matrix as a heatmap. Recall that $D_{\\mathrm{KL}}(P\\|Q)$ measures the amount of information loss when $Q$ is used to approximate $P$. Here, the rows are the $P$s used for calculating KL divergences, and the columns are the $Q$s. So, each cell measures the amount of information loss when the word distribution of the column text is used to approximate the word distribution of the row text. Because the KL divergence is directional, such that the divergence of $P$ from $Q$ is different from the same of $Q$ from $P$, the matrix is assymetric and contains unique information above and below the diagonal. The same is true for the $\\chi^2$ divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora:\n",
    "    l = []\n",
    "    for q in corpora:\n",
    "        l.append(Divergence(p,q, difference = 'KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that works by the same author have the lowest within-group KL divergences. \n",
    "\n",
    "To reveal more patterns, let's do a multidimensional scaling of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds = sklearn.manifold.MDS()\n",
    "pos = mds.fit(M).embedding_\n",
    "x = pos[:,0]\n",
    "y = pos[:,1]\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "plt.plot(x, y, ' ')\n",
    "for i, txt in enumerate(fileids):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any patterns in the image shown above? Does it make sense?\n",
    "\n",
    "We may just want to focus on the distrbution of stopwords or non-stopwords. Let's do the analysis again first for stopwords and then for non-stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_s:\n",
    "    l = []\n",
    "    for q in corpora_s:\n",
    "        l.append(Divergence(p,q, difference='KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the same for the assymmetric $\\chi^2$ Divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='Chi2'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "For the KS distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='KS'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally for the Wasserstein or \"earth mover's\" Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='Wasserstein'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the Bible is highly divergent and this makes reading the plot somewhat difficult. Let's fix this by taking the log of each cell, which will reduce the distance of the Bible from other texts as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(div.apply(np.log).replace([np.inf, -np.inf], np.nan))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more legible, as it continues to show that the Bible is divergent, but other structure is also visible, such as the divergence of Austen's *Emma* and *Sense and Sensibility*, and Edgeworth's *The Parent's Assistant*. \n",
    "\n",
    "If we want to rerun this on a new data set of our own composition, we can be a bit more efficient with our coding. Let's use the Shakespeare texts from last week as example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_df = lucem_illud.loadTextDirectory('../data/Shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stem and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_df['tokenized_text'] = shakespeare_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "shakespeare_df['normalized_tokens'] = shakespeare_df['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = porter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the corpus file and generate the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measure = 'KS' #\"KL\", \"Chi2\", \"KS\", \"Wasserstein\"\n",
    "num_of_texts = 10 #The bigger this number the slower it will run, you can also try selecting your own plays\n",
    "fileids_sp = list(shakespeare_df[:num_of_texts].index)\n",
    "corpora_sp = list(shakespeare_df[:num_of_texts]['normalized_tokens'])\n",
    "L = []\n",
    "for p in corpora_sp:\n",
    "    l = []\n",
    "    for q in corpora_sp:\n",
    "        l.append(Divergence(p,q, difference=measure))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids_sp, index = fileids_sp)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are \"The Passionate Pilgrim\" and \"The Phoenix and the Turtle\"? Little known poems by Shakespeare that are unsurprisingly hard to classify, as they are so different from everything else he wrote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate the KL and $\\chi^2$ divergences, and the KS and Wasserstein distances between four separate corpora, plot these with heatmaps, and then array them in two dimensions with multidimensional scaling as above. What does this reveal about relations between the corpora? Which analysis (and divergence or distribution) distinguishes the authors or documents better? \n",
    "\n",
    "<span style=\"color:red\">**Stretch**: Calculate the <a \"href=https://en.wikipedia.org/wiki/Jensen–Shannon_divergence\" target=\"_blank\">Jensen-Shannon Divergence</a> between your four corpora. What is the relationship between the KL and JS divergences?</span> "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
