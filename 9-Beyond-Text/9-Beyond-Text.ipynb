{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Beyond Text\n",
    "\n",
    "This week, we \"trascend\" text to explore analysis of sound and visual content. Trillions of digital audio, image, and video files have been generated by cell phones and distributed sensors, preserved and shared through social medial, the web, private and government administrations. In this notebook, we read in and visualize audio and image files, process them to extract relevant features and measurement, then begin to explore how to analyze and extract information from them through the same approaches to supervised and unsupervised learning we have performed thoughout the quarter with text.\n",
    "\n",
    "For this notebook we will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import scipy #For frequency analysis\n",
    "import scipy.fftpack\n",
    "import nltk #the Natural Language Toolkit\n",
    "import requests #For downloading our datasets\n",
    "import numpy as np #for arrays\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import IPython #To show stuff\n",
    "\n",
    "#Image handling install as Pillow\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "\n",
    "#install as scikit-image, this does the image manupulation\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.segmentation\n",
    "import skimage.filters\n",
    "import skimage.color\n",
    "import skimage.graph\n",
    "import skimage.future.graph\n",
    "\n",
    "#these three do audio handling\n",
    "import pydub #Requires ffmpeg to be installed https://www.ffmpeg.org/download.html; on a mac \"brew install ffmpeg\"\n",
    "import speech_recognition #install as speechrecognition\n",
    "import soundfile #Install as pysoundfile \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning it may generate.\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio analysis \n",
    "\n",
    "First we will consider media that predates written language...sound and spoken language. Audio (and video) files come in two major categories, lossy or lossless. Lossless files save all information the microphone recorded. Lossy files, by contrast, drop sections humans are unlikely to notice. Recorded frequencies for both types are then typically compressed, which introduces further loss. To work with audio files, we want a format that is preferably lossless or minimally compressed. We will work with `wav` files here. Note that `mp3` is not acceptable. If you do not have `wav` files, we can use python to convert to `wav`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samplePath = '../data/audio_samples/SBC060.mp3'\n",
    "transcriptPath = '../data/audio_samples/SBC060.trn'\n",
    "\n",
    "IPython.display.Audio(samplePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are using a different package to convert than the in the rest of the code\n",
    "def convertToWAV(sourceFile, outputFile, overwrite = False):\n",
    "    if os.path.isfile(outputFile) and not overwrite:\n",
    "        print(\"{} exists already\".format(outputFile))\n",
    "        return\n",
    "    #Naive format extraction\n",
    "    sourceFormat = sourceFile.split('.')[-1]\n",
    "    sound = pydub.AudioSegment.from_file(sourceFile, format=sourceFormat)\n",
    "    sound.export(outputFile, format=\"wav\")\n",
    "    print(\"{} created\".format(outputFile))\n",
    "wavPath = 'sample.wav'\n",
    "convertToWAV(samplePath, wavPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our `wav` file, notice that it is much larger than the source `mp3`. We can load it with `soundfile` and work with it as a numpy data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soundArr, soundSampleRate = soundfile.read(wavPath)\n",
    "soundArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the raw data as a column array, which contains two channels (Left and Right) of the recording device. Some files, of course, will have more columns (from more microphones). The array comprises a series of numbers that measure the location of the speaker membrane (0=resting location). By quickly and rhythmically changing the location a note can be achieved. The larger the variation from the center, the louder the sound; the faster the oscillations, the higher the pitch. (The center of the oscillations does not have to be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soundSampleRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other piece of information we get is the sample rate. This tells us how many measurements made per second, which allows us to know how long the entire recording is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numS = soundArr.shape[0] // soundSampleRate\n",
    "print(\"The sample is {} seconds long\".format(numS))\n",
    "print(\"Or {:.2f} minutes\".format(numS / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final critical parameter of sound digitization is quantisation, which consists in assigning a value to each sample according to its amplitude. These values are attributed according to a bit scale. A quantisation of 8 bit will assign amplitude values along a scale of $2^8 = 256$ states around 0. Most recording systems use a $2^{16} = 65536$ bit system. Quantisation is a rounding process, where high bit quantisation produces values close to reality with values rounded to a high number of significant digits, and low bit quantisation produces values further from reality with values rounded a low number of significants digits. Low quantisation can lead to impaired quality signal. <img src=\"../data/bitrate.png\"> This figure illustrates how digital sounds is a discrete process along the amplitude scale: a 3 bit, $2^3=8$, quantization (gray bars) gives a rough approximation of the sin wave (red line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first second of the recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(soundArr[:soundSampleRate])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 2 (Left and Right) nearly \"flat\" (or equally wavy) lines. This means that there is very little noise at this part of the recording. What variation exists is due to compression or interference and represents the slight hiss you sometimes hear in low quality recordings.\n",
    "\n",
    "Let's expand our scope and look at the first 10 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soundArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(soundArr[:soundSampleRate * 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see definite spikes, where each represents a word or discrete sound.\n",
    "\n",
    "To see what the different parts correspond to, we can use a transcript. Because we got this file from the [Santa Barbara Corpus of Spoken American English\n",
    "](http://www.linguistics.ucsb.edu/research/santa-barbara-corpus#Contents), we just need to load the metadata, which includes a transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadTranscript(targetFile):\n",
    "    #Regex because the transcripts aren't consistent enough to use csv\n",
    "    regex = re.compile(r\"(\\d+\\.\\d+)\\s(\\d+\\.\\d+)\\s(.+:)?\\s+(.*)\")\n",
    "    dfDict = {\n",
    "        'time_start' : [],\n",
    "        'time_end' : [],\n",
    "        'speaker' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "    with open(targetFile, encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            r = re.match(regex, line)\n",
    "            dfDict['time_start'].append(float(r.group(1)))\n",
    "            dfDict['time_end'].append(float(r.group(2)))\n",
    "            if r.group(3) is None:\n",
    "                dfDict['speaker'].append(dfDict['speaker'][-1])\n",
    "            else:\n",
    "                dfDict['speaker'].append(r.group(3))\n",
    "            dfDict['text'].append(r.group(4))\n",
    "    return pandas.DataFrame(dfDict)\n",
    "\n",
    "transcriptDF = loadTranscript(transcriptPath)\n",
    "transcriptDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a few sub-sections. First, to make things easier, we will convert the seconds markers to sample indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to be ints for indexing, luckily being off by a couple indices doesn't matter\n",
    "transcriptDF['index_start'] = (transcriptDF['time_start'] * soundSampleRate).astype('int')\n",
    "transcriptDF['index_end'] = (transcriptDF['time_end'] * soundSampleRate).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what `'Rae and I and Sue and Buddy,'` looks like, which is the seventh row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "subSample1 = soundArr[transcriptDF['index_start'][6]: transcriptDF['index_end'][6]]\n",
    "ax.plot(subSample1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's hear what that sounds like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soundfile.write('../data/audio_samples/sample1.wav', subSample1, soundSampleRate)\n",
    "IPython.display.Audio('../data/audio_samples/sample1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see sounds in the frequency space, we can take the Fourier transform. This is a reversible mathematical transform named after the French mathematician Joseph Fourier (1768-1830) <img src=\"data/Fourier.jpg\">. The transform decomposes a time series into a sum of finite series of sine or cosine functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample1FFT = scipy.fftpack.ifft(subSample1)\n",
    "N = len(sample1FFT)\n",
    "freq = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.set_xlabel('Frequency ($Hz$)')\n",
    "ax.set_ylabel('Intensity')\n",
    "ax.plot(freq[:N//2], abs(sample1FFT)[:N//2]) #Only want positive frequencies\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are two frequencies to the 'Rae and I and Sue and Buddy' snippet: a higher pitched 'Rae and I...Sue...Buddy' (~14000 *Hz*) and the final two 'and's (one at ~6000 *Hz* and the second at ~8000 *Hz*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a sniff look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "subSample2 = soundArr[transcriptDF['index_start'][9]: transcriptDF['index_end'][9]]\n",
    "ax.plot(subSample2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very different from speech. And now let's see what that sounds like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soundfile.write('../data/audio_samples/sample2.wav', subSample2, soundSampleRate)\n",
    "IPython.display.Audio('../data/audio_samples/sample2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in frequency space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample2FFT = scipy.fftpack.ifft(subSample2)\n",
    "N = len(sample2FFT)\n",
    "freq = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(freq[:N//2], abs(sample2FFT)[:N//2]) #Only want positive frequencies\n",
    "ax.set_xlabel('Frequency ($Hz$)')\n",
    "ax.set_ylabel('Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there is not a dominant frequency for the sniff as there was for the noun phrase earlier. This means that the sniff activated noise all across the frequency spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also investigate dominant frequencies for the entire record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This takes a while\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "fullFFT = scipy.fftpack.ifft(soundArr)\n",
    "N = len(fullFFT)\n",
    "freq = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "ax.plot(freq[:N//2], abs(fullFFT)[:N//2]) #Only want positive frequencies\n",
    "ax.set_xlabel('Frequency ($Hz$)')\n",
    "ax.set_ylabel('Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq[len(freq) // 2 -10: len(freq) // 2 + 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we capture each person's frequencies across their entire collection of statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxfreq(sample, topN = 10):\n",
    "    sampleFFT = scipy.fftpack.ifft(sample)\n",
    "    N = len(sample)\n",
    "    freqs = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "    tops =  np.argpartition(abs(sampleFFT[:, 0]), -topN)[-topN:]\n",
    "\n",
    "    return np.mean(tops) \n",
    "\n",
    "freqs = []\n",
    "for i, row in transcriptDF.iterrows():\n",
    "    freqs.append(maxfreq(soundArr[row['index_start']: row['index_end']]))\n",
    "\n",
    "transcriptDF['frequency FFT'] = freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alan's speech exhibits the following frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "transcriptDF[transcriptDF['speaker'] == 'ALAN:'].plot( 'time_start', 'frequency FFT', ax = ax)\n",
    "ax.set_ylabel(\"Frequency FFT space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...while Jon's voice is **much** lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "transcriptDF[transcriptDF['speaker'] == 'JON:'].plot( 'time_start', 'frequency FFT', ax = ax)\n",
    "ax.set_ylabel(\"Frequency FFT space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can look at them togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fg = seaborn.FacetGrid(data=transcriptDF, hue='speaker', aspect = 3)\n",
    "fg.map(plt.scatter, 'time_start', 'frequency FFT').add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that read in 10 audio files (e.g., produced on your smartphone recorder?) from at least two different speakers, which include sentences of different types (e.g., question, statement, exclamation). At least two of these should include recordings of the two speakers talking to each other (e.g., a simple question/answer). Contrast the frequency distributions of the words spoken within speaker. What speaker's voice has a higher and which has lower frequency? What words are spoken at the highest and lowest frequencies? What parts-of-speech tend to be high or low? How do different types of sentences vary in their frequency differently? When people are speaking to each other, how do their frequencies change? Whose changes more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text\n",
    "\n",
    "We can also do speech recognition on audio, but this requires a complex machine learning system. Luckily there are many online services to do this. We have a function that uses Google's API. There are two API's: one is free but limited; the other is commercial and you can provide the function `speechRec` with a file containing the API keys, using `jsonFile=` if you wish. For more about this look [here](https://stackoverflow.com/questions/38703853/how-to-use-google-speech-recognition-api-in-python) or the `speech_recognition` [docs](https://github.com/Uberi/speech_recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using another library so we need to use files again\n",
    "def speechRec(targetFile, language = \"en-US\", raw = False, jsonFile = ''):\n",
    "    r = speech_recognition.Recognizer()\n",
    "    if not os.path.isfile(jsonFile):\n",
    "        jsonString = None\n",
    "    else:\n",
    "        with open(jsonFile) as f:\n",
    "            jsonString = f.read()\n",
    "    with speech_recognition.AudioFile(targetFile) as source:\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        if jsonString is None:\n",
    "            print(\"Sending data to Google Speech Recognition\")\n",
    "            dat =  r.recognize_google(audio)\n",
    "        else:\n",
    "            print(\"Sending data to Google Cloud Speech\")\n",
    "            dat =  r.recognize_google_cloud(audio, credentials_json=jsonString)\n",
    "    except speech_recognition.UnknownValueError:\n",
    "        print(\"Google could not understand audio\")\n",
    "    except speech_recognition.RequestError as e:\n",
    "        print(\"Could not request results from Google service; {0}\".format(e))\n",
    "    else:\n",
    "        print(\"Success\")\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is of too low quality so we will be using another file `data/audio_samples/english.wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio('../data/audio_samples/english.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speechRec('../data/audio_samples/english.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that use the 10 audio files from at least two different speakers read in previously, attempt to automatically extract the words from Google, and calculate the word-error rate, as descibed in Chapter 9 from *Jurafsky & Martin*, page 334. How well does it do? Under what circumstances does it perform poorly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image analysis\n",
    "\n",
    "Now we will explore image files. First, we will read in a couple of images. Please change the working image and see how the resuts differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_wiki = PIL.Image.open('../data/Wikimedia_Conference_2016_–_Group_photo.jpg')\n",
    "image_wikiGray = PIL.ImageOps.invert(image_wiki.convert('L'))\n",
    "\n",
    "image_AllSaints = PIL.Image.open('../data/AllSaintsMargaretStreet-DAVID_ILIFF.jpg')\n",
    "image_AllSaintsGray = PIL.ImageOps.invert(image_AllSaints.convert('L'))\n",
    "\n",
    "image_Soyuz = PIL.Image.open('../data/Soyuz.jpg')\n",
    "image_SoyuzGray = PIL.ImageOps.invert(image_Soyuz.convert('L'))\n",
    "\n",
    "image_Rock = PIL.Image.open('../data/Bi-crystal.jpg')\n",
    "image_RockGray = PIL.ImageOps.invert(image_Rock.convert('L'))\n",
    "\n",
    "image_flowers = PIL.Image.open('../data/flowers.jpg')\n",
    "image_flowersGray = PIL.ImageOps.invert(image_flowers.convert('L'))\n",
    "\n",
    "image = image_flowers\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageArr = np.asarray(image)\n",
    "imageArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image we have loaded is a raster image, meaning it is a grid of pixels. Each pixel contains 1-4 numbers giving the amounts of color contained in it. In this case, we can see it has 3 values per pixel, these are RGB or Red, Green and Blue values. If we want to see just the red, we can look at just that array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(imageArr[:,:,0], cmap='Reds') #The order is R G B, so 0 is the Reds\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(imageArr[:,:,1], cmap='Greens') #The order is R G B, so 2 is the Green\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(imageArr[:,:,2], cmap='Blues') #The order is R G B, so 2 is the Blue\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can look at all four together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (10, 10), ncols=2, nrows=2)\n",
    "axeIter = iter(axes.flatten())\n",
    "colours = [\"Reds\", \"Greens\", \"Blues\"]\n",
    "ax = next(axeIter)\n",
    "ax.imshow(imageArr)\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(3):\n",
    "    ax = next(axeIter)\n",
    "    ax.imshow(imageArr[:,:,i], cmap=colours[i]) #The order is R G B, so 2 is the Blue\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_gray = PIL.ImageOps.invert(image.convert('L'))\n",
    "image_grayArr = np.asarray(image_gray)\n",
    "image_grayArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grayscale image is defined by its pixel intensities (and a color image can be defined by its red, green, blue pixel intensities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(image_grayArr) #No third dimension\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob Detection\n",
    "\n",
    "Recall our earlier use of scikit-learn for machine learning. Now we will use scikit-image to do some simple image processing. Here we will perform three operations for 'blob' of simple object detection. In computer vision, blob detection methods aim to detect regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. Informally, a blob is a region of an image in which some properties are approximately constant or similar to each other. We will do this in three ways.\n",
    "\n",
    "First, we will take the Laplacian of an image, which is a 2-D isotropic (applying equally well in all directions) measure of the 2nd spatial derivative of an image. The Laplacian of an image highlights regions of rapid intensity change and is therefore often used for edge detection. This Laplacian is taken of the image once a Gaussian smoothing filter has been applied in order to reduce its sensitivity to noise.\n",
    "\n",
    "The Laplacian $L(x,y)$ of an image with pixel intensity values $I(x,y)$ is given by: $L(x,y)=\\frac{\\delta^2x}{\\delta x^2} + \\frac{\\delta^2y}{\\delta y^2}$. A Gaussian smoothing filter takes a 2 dimensional Guassian, $G(x,y)=\\frac{1}{2 \\pi \\sigma^2} e^\\frac{-x^2 + y^2}{2\\sigma^2}$, which looks like: <img src=\"http://www.librow.com/content/common/images/articles/article-9/2d_distribution.gif\">\n",
    "\n",
    "This Gaussian *kernel* is applied to the pixel intensities of the image via *convolution* -- the kernel is multiplied by the pixel intensities, while centered on each pixel, then added.\n",
    "\n",
    "The blob detector computes the [Laplacian of Gaussian (LoG)](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_log) images with successively increasing standard deviation and stacks them up in a cube. Blobs are local maximas within this cube. Detecting larger blobs is slower because of larger kernel sizes during convolution. Bright blobs on dark backgrounds are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobs_log = skimage.feature.blob_log(image_grayArr, max_sigma=30, num_sigma=5, threshold=.1)\n",
    "blobs_log[:, 2] = blobs_log[:, 2] * np.sqrt(2) #Radi\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.axis('off')\n",
    "\n",
    "plt.imshow(image_grayArr, interpolation='nearest')\n",
    "for blob in blobs_log:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we look at [Difference of Gaussian (DoG)](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog), a much faster approximation of the LoG approach in which an image is blurred with increasing standard deviations and the difference between two successively blurred images are stacked up in a cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobs_dog = skimage.feature.blob_dog(image_grayArr, max_sigma=30, threshold=.1)\n",
    "blobs_dog[:, 2] = blobs_dog[:, 2] * np.sqrt(2)\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.axis('off')\n",
    "\n",
    "plt.imshow(image_grayArr, interpolation='nearest')\n",
    "for blob in blobs_dog:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we consider the [Determinant of Hessian (DoH)](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_doh) approach. The Hessian matrix or Hessian is a square matrix of second-order partial derivatives $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x_1^{*}, \\ldots, x_n^{*})$ and is calculated on square pixel patches of the image. The determinant is the scaling factor of each patch. This approach is fastest and detects blobs by finding maximas in this matrix (of the Determinant of the Hessian of the image). Detection speed is independent of the size of blobs as the implementation uses box filters, $\\begin{bmatrix}1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1\\end{bmatrix}$, instead of Gaussians for the convolution. As a result, small blobs (< 3 pixels) cannot be detected accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobs_doh = skimage.feature.blob_doh(image_grayArr, max_sigma=30, threshold=.01)\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.axis('off')\n",
    "\n",
    "plt.imshow(image_grayArr, interpolation='nearest')\n",
    "for blob in blobs_doh:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans possess an incredible ability to identify objects in an image. Segmentation is the process of dividing an image into meaningful regions. All pixels belonging to a region should receive a unique label in an ideal segmentation.\n",
    "\n",
    "Region Adjacency Graphs (RAGs) are a common data structure for many segmentation algorithms. First, we define regions through the SLIC algorithm that assigns a unique label to each region or a localized cluster of pixels sharing some similar property (e.g., color or grayscale intensity). Then we'll consider each region a node in a graph, and construct a region boundary RAG, where the edge weight between two regions is the average value of the corresponding pixels in edge_map along their shared boundary. Then edges below a specified threshold are removed and a connected component is labeled as one region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = skimage.segmentation.slic(image_grayArr, compactness=30, n_segments=400)\n",
    "g = skimage.future.graph.rag_mean_color(image_grayArr, labels)\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.axis('off')\n",
    "lc = skimage.future.graph.show_rag(labels, g, image_grayArr, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting boundaries were constructed for this image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize many of the approaches into a collection of kernel methods. These are also how Convolutional Neural Networks (CNNs) create their features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeConvo(a_c):\n",
    "    s = np.sum(a_c.flatten())\n",
    "    return a_c / s\n",
    "\n",
    "def displayConvolution(img, convArray, threshold = None):\n",
    "    img_array = np.asarray(img).astype('uint8')\n",
    "    \n",
    "    if len(img_array.shape) > 2:\n",
    "        conv = np.zeros(img_array.shape)\n",
    "        for i in range(img_array.shape[2]):\n",
    "            conv[:,:,i] = scipy.ndimage.convolve(img_array[:,:,i], normalizeConvo(convArray), mode='constant')\n",
    "    else:\n",
    "        conv = scipy.ndimage.convolve(img_array, normalizeConvo(convArray), mode='constant')\n",
    "    conv = conv.astype('uint8')\n",
    "    if threshold is not None:\n",
    "        if threshold < 1:\n",
    "            threshold = threshold * 255\n",
    "        conv[conv < threshold] = 0\n",
    "    \n",
    "    conv_image = PIL.Image.fromarray(conv)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (17, 10))\n",
    "    #This is to deal with some annoying plt/PIL stuff\n",
    "    if len(img_array.shape) > 2:\n",
    "        ax1.imshow(img)\n",
    "    else:\n",
    "        ax1.imshow(img_array)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original\")\n",
    "    #return conv_image\n",
    "    if len(img_array.shape) > 2:\n",
    "        ax2.imshow(conv_image)\n",
    "    else:\n",
    "        ax2.imshow(np.asarray(conv_image))\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Convolution\")\n",
    "    \n",
    "    diff = PIL.ImageChops.difference(conv_image, img)\n",
    "    if len(img_array.shape) > 2:\n",
    "        ax3.imshow(diff)\n",
    "    else:\n",
    "        ax3.imshow(np.asarray(diff))\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title(\"Difference\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #Not returning anything to make displaying nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://en.wikipedia.org/wiki/Kernel_(image_processing%29) are a fistfull kenels to try. In image processing, kernels (also convolution matrices or masks) are small matrices. They are used for blurring, sharpening, edge detection, and more. This is accomplished by doing a convolution between a kernel and an image by adding each element of the image to its local neighbors, weighted by the kernel, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoothingKernel = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 1],\n",
    "    [1, 1, 1]])\n",
    "c = displayConvolution(image_gray, smoothingKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verticalLineKernel = np.array([\n",
    "    [1, 0, -1],\n",
    "    [0, 8, 0],\n",
    "    [1, 0, -1]])\n",
    "displayConvolution(image, verticalLineKernel, threshold=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diagonalLineKernel = np.array([\n",
    "    [-1, 0, -1],\n",
    "    [0, 8, 0],\n",
    "    [1, 0, -1]])\n",
    "displayConvolution(image, diagonalLineKernel, threshold=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blurKernel = np.array([\n",
    "    [1, 2, 4, 2, 1],\n",
    "    [2, 4, 8, 4, 2],\n",
    "    [4, 8, 16, 8, 4],\n",
    "    [2, 4, 8, 4, 2],\n",
    "    [1, 2, 4, 2, 1]])\n",
    "displayConvolution(image, blurKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SharpenKernel = np.array([\n",
    "    [ 0,  0,  -1,  0,  0],\n",
    "    [ 0, -1,  -2, -1,  0],\n",
    "    [-1, -2,  20, -2, -1],\n",
    "    [ 0, -1,  -2, -1,  0],\n",
    "    [ 0,  0,  -1,  0,  0]])\n",
    "displayConvolution(image, SharpenKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "differentKernel = np.array([\n",
    "    [ 1, 0,  0, 0,  -1],\n",
    "    [ 1, 0,  0, 0,  -1],\n",
    "    [ 1, 0,  1, 0,  -1],\n",
    "    [ 1, 0,  0, 0,  -1],\n",
    "    [ 1, 0,  0, 0,  -1]])\n",
    "#This looks neat\n",
    "displayConvolution(image, differentKernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that read in 10 image files (e.g., produced on your smartphone, harvested from the web, etc.) that feature different kinds of objects and settings, including at least one indoor and one outdoor setting. Perform blob detection and RAG segmentation using the approaches modeled above. How well does each algorithm identify objects or features of interest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection & Scene Classification\n",
    "\n",
    "Modern image and video analysis is typically performed using deep learning implemented as layers of convolutional neural nets to classify scenes and to detect and label objects. To learn more about deep learning and convolutional neural networks, spend some time with Andrew Ng's excellent [tutorial](http://ufldl.stanford.edu/tutorial/). Because such algorithms require substantial computing power, none of the high-quality classifiers or detectors currently available are implemented in python, although many can be called via api. The most popular open source image object detector is UC Berkeley's [*caffe*](http://caffe.berkeleyvision.org) library of trained and trainable neural nets written in C++. (Check out the [python api](https://github.com/BVLC/caffe/blob/master/python/caffe/pycaffe.py)). Scene classifiers can be built on top of caffe, such as MIT's [Places](http://places2.csail.mit.edu/demo.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that report the results from experiments in which you place each of images taken or retrieved for the last exercise through the online demos for [caffe](http://demo.caffe.berkeleyvision.org) and [places](http://places.csail.mit.edu/demo.html). Paste the image and the output for both object detector and scene classifier below, beside one another. Calculate precision and recall for caffe's ability to detect objects of interest across your images. What do you think about Places' scene categories and their assignments to your images? What would be improved labels for your images? Could you use image classification to enhance your research project and, if so, how?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
